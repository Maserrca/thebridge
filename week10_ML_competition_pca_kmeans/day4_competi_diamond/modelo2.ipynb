{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit"
  },
  "interpreter": {
   "hash": "797ece2db6cd49410b212500b07c3b6429b528fb918ab6c0d0ffbc4c51c5b09e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       carat        cut color clarity  depth  table     x     y     z  price\n",
       "0       1.21      Ideal     H     VS2   63.0   57.0  6.73  6.70  4.23   6134\n",
       "1       0.28  Very Good     D    VVS2   64.0   56.0  4.14  4.17  2.66    532\n",
       "2       0.42    Premium     F     VS1   61.2   58.0  4.86  4.82  2.96   1103\n",
       "3       0.26      Ideal     H      IF   61.1   57.0  4.16  4.12  2.53    600\n",
       "4       1.10       Good     G     SI1   63.4   57.0  6.52  6.55  4.14   4997\n",
       "...      ...        ...   ...     ...    ...    ...   ...   ...   ...    ...\n",
       "40340   1.55    Premium     H     VS2   61.3   61.0  7.46  7.39  4.55  11708\n",
       "40341   0.36      Ideal     D     SI1   60.6   56.0  4.58  4.63  2.79    619\n",
       "40342   0.57  Very Good     I     VS2   62.2   55.0  5.33  5.34  3.32   1267\n",
       "40343   1.01  Very Good     F      IF   59.6   62.0  6.47  6.56  3.88   9965\n",
       "40344   0.54      Ideal     E     SI2   60.4   57.0  5.33  5.27  3.20   1340\n",
       "\n",
       "[40345 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.21</td>\n      <td>Ideal</td>\n      <td>H</td>\n      <td>VS2</td>\n      <td>63.0</td>\n      <td>57.0</td>\n      <td>6.73</td>\n      <td>6.70</td>\n      <td>4.23</td>\n      <td>6134</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.28</td>\n      <td>Very Good</td>\n      <td>D</td>\n      <td>VVS2</td>\n      <td>64.0</td>\n      <td>56.0</td>\n      <td>4.14</td>\n      <td>4.17</td>\n      <td>2.66</td>\n      <td>532</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.42</td>\n      <td>Premium</td>\n      <td>F</td>\n      <td>VS1</td>\n      <td>61.2</td>\n      <td>58.0</td>\n      <td>4.86</td>\n      <td>4.82</td>\n      <td>2.96</td>\n      <td>1103</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.26</td>\n      <td>Ideal</td>\n      <td>H</td>\n      <td>IF</td>\n      <td>61.1</td>\n      <td>57.0</td>\n      <td>4.16</td>\n      <td>4.12</td>\n      <td>2.53</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.10</td>\n      <td>Good</td>\n      <td>G</td>\n      <td>SI1</td>\n      <td>63.4</td>\n      <td>57.0</td>\n      <td>6.52</td>\n      <td>6.55</td>\n      <td>4.14</td>\n      <td>4997</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40340</th>\n      <td>1.55</td>\n      <td>Premium</td>\n      <td>H</td>\n      <td>VS2</td>\n      <td>61.3</td>\n      <td>61.0</td>\n      <td>7.46</td>\n      <td>7.39</td>\n      <td>4.55</td>\n      <td>11708</td>\n    </tr>\n    <tr>\n      <th>40341</th>\n      <td>0.36</td>\n      <td>Ideal</td>\n      <td>D</td>\n      <td>SI1</td>\n      <td>60.6</td>\n      <td>56.0</td>\n      <td>4.58</td>\n      <td>4.63</td>\n      <td>2.79</td>\n      <td>619</td>\n    </tr>\n    <tr>\n      <th>40342</th>\n      <td>0.57</td>\n      <td>Very Good</td>\n      <td>I</td>\n      <td>VS2</td>\n      <td>62.2</td>\n      <td>55.0</td>\n      <td>5.33</td>\n      <td>5.34</td>\n      <td>3.32</td>\n      <td>1267</td>\n    </tr>\n    <tr>\n      <th>40343</th>\n      <td>1.01</td>\n      <td>Very Good</td>\n      <td>F</td>\n      <td>IF</td>\n      <td>59.6</td>\n      <td>62.0</td>\n      <td>6.47</td>\n      <td>6.56</td>\n      <td>3.88</td>\n      <td>9965</td>\n    </tr>\n    <tr>\n      <th>40344</th>\n      <td>0.54</td>\n      <td>Ideal</td>\n      <td>E</td>\n      <td>SI2</td>\n      <td>60.4</td>\n      <td>57.0</td>\n      <td>5.33</td>\n      <td>5.27</td>\n      <td>3.20</td>\n      <td>1340</td>\n    </tr>\n  </tbody>\n</table>\n<p>40345 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "df = pd.read_csv(\"diamonds_train.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       carat  cut  color  clarity  depth  table     x     y     z  price\n",
       "0       1.21    2      4        5   63.0   57.0  6.73  6.70  4.23   6134\n",
       "1       0.28    4      0        7   64.0   56.0  4.14  4.17  2.66    532\n",
       "2       0.42    3      2        4   61.2   58.0  4.86  4.82  2.96   1103\n",
       "3       0.26    2      4        1   61.1   57.0  4.16  4.12  2.53    600\n",
       "4       1.10    1      3        2   63.4   57.0  6.52  6.55  4.14   4997\n",
       "...      ...  ...    ...      ...    ...    ...   ...   ...   ...    ...\n",
       "40340   1.55    3      4        5   61.3   61.0  7.46  7.39  4.55  11708\n",
       "40341   0.36    2      0        2   60.6   56.0  4.58  4.63  2.79    619\n",
       "40342   0.57    4      5        5   62.2   55.0  5.33  5.34  3.32   1267\n",
       "40343   1.01    4      2        1   59.6   62.0  6.47  6.56  3.88   9965\n",
       "40344   0.54    2      1        3   60.4   57.0  5.33  5.27  3.20   1340\n",
       "\n",
       "[40345 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>carat</th>\n      <th>cut</th>\n      <th>color</th>\n      <th>clarity</th>\n      <th>depth</th>\n      <th>table</th>\n      <th>x</th>\n      <th>y</th>\n      <th>z</th>\n      <th>price</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1.21</td>\n      <td>2</td>\n      <td>4</td>\n      <td>5</td>\n      <td>63.0</td>\n      <td>57.0</td>\n      <td>6.73</td>\n      <td>6.70</td>\n      <td>4.23</td>\n      <td>6134</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.28</td>\n      <td>4</td>\n      <td>0</td>\n      <td>7</td>\n      <td>64.0</td>\n      <td>56.0</td>\n      <td>4.14</td>\n      <td>4.17</td>\n      <td>2.66</td>\n      <td>532</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.42</td>\n      <td>3</td>\n      <td>2</td>\n      <td>4</td>\n      <td>61.2</td>\n      <td>58.0</td>\n      <td>4.86</td>\n      <td>4.82</td>\n      <td>2.96</td>\n      <td>1103</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.26</td>\n      <td>2</td>\n      <td>4</td>\n      <td>1</td>\n      <td>61.1</td>\n      <td>57.0</td>\n      <td>4.16</td>\n      <td>4.12</td>\n      <td>2.53</td>\n      <td>600</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.10</td>\n      <td>1</td>\n      <td>3</td>\n      <td>2</td>\n      <td>63.4</td>\n      <td>57.0</td>\n      <td>6.52</td>\n      <td>6.55</td>\n      <td>4.14</td>\n      <td>4997</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40340</th>\n      <td>1.55</td>\n      <td>3</td>\n      <td>4</td>\n      <td>5</td>\n      <td>61.3</td>\n      <td>61.0</td>\n      <td>7.46</td>\n      <td>7.39</td>\n      <td>4.55</td>\n      <td>11708</td>\n    </tr>\n    <tr>\n      <th>40341</th>\n      <td>0.36</td>\n      <td>2</td>\n      <td>0</td>\n      <td>2</td>\n      <td>60.6</td>\n      <td>56.0</td>\n      <td>4.58</td>\n      <td>4.63</td>\n      <td>2.79</td>\n      <td>619</td>\n    </tr>\n    <tr>\n      <th>40342</th>\n      <td>0.57</td>\n      <td>4</td>\n      <td>5</td>\n      <td>5</td>\n      <td>62.2</td>\n      <td>55.0</td>\n      <td>5.33</td>\n      <td>5.34</td>\n      <td>3.32</td>\n      <td>1267</td>\n    </tr>\n    <tr>\n      <th>40343</th>\n      <td>1.01</td>\n      <td>4</td>\n      <td>2</td>\n      <td>1</td>\n      <td>59.6</td>\n      <td>62.0</td>\n      <td>6.47</td>\n      <td>6.56</td>\n      <td>3.88</td>\n      <td>9965</td>\n    </tr>\n    <tr>\n      <th>40344</th>\n      <td>0.54</td>\n      <td>2</td>\n      <td>1</td>\n      <td>3</td>\n      <td>60.4</td>\n      <td>57.0</td>\n      <td>5.33</td>\n      <td>5.27</td>\n      <td>3.20</td>\n      <td>1340</td>\n    </tr>\n  </tbody>\n</table>\n<p>40345 rows × 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "def codi(df):\n",
    "    '''\n",
    "    Esta función da valores numéricos a las columnas cut, color y clarity\n",
    "    '''\n",
    "    cols = ['cut', 'color', 'clarity']\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col].astype('str'))\n",
    "    return df\n",
    "codi(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(df[['carat','cut', 'color', 'clarity']])\n",
    "y = np.array(df[\"price\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "# Grid to search for the best random tree model\n",
    "def random_tree_grid(X,y):\n",
    "    rfc = RandomForestRegressor(n_jobs=-1,max_features= 'sqrt'\n",
    "                                 ,n_estimators=50, criterion = \"mse\") \n",
    "    \n",
    "    param_grid = { \n",
    "        'n_estimators': [10,25,40,50,60,75,100],\n",
    "        'min_samples_split': [5,25,50,100],\n",
    "        'min_samples_leaf': [5,25,50,100],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    CV_rfc = GridSearchCV(estimator=rfc,verbose=1, param_grid=param_grid, cv= 5)\n",
    "    CV_rfc.fit(X, y)\n",
    "    \n",
    "    rfc = RandomForestRegressor(n_jobs=-1,**CV_rfc.best_params_, criterion = \"mse\")\n",
    "    rfc.fit(X,y)\n",
    "    \n",
    "    return rfc\n",
    "\n",
    "# Improved grid to search for the best random tree model\n",
    "def tuning_random_tree_grid(X,y):\n",
    "    rfc = RandomForestRegressor(n_jobs=-1,max_features= 'sqrt'\n",
    "                                 ,n_estimators=50, criterion = \"mse\") \n",
    "    \n",
    "    param_grid = { \n",
    "        'max_depth': [25,50,100,None],\n",
    "        'min_samples_split': [2,5,10],\n",
    "        'min_samples_leaf': [1,3,5],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'n_estimators': [50,250, 1000, 1240, 1500, 2000]\n",
    "    }\n",
    " \n",
    "    CV_rfc = GridSearchCV(estimator=rfc,verbose=3, param_grid=param_grid, cv= 6)\n",
    "    CV_rfc.fit(X, y)\n",
    "    \n",
    "    rfc = RandomForestRegressor(n_jobs=-1,**CV_rfc.best_params_, criterion = \"mse\")\n",
    "    rfc.fit(X,y)\n",
    "    \n",
    "    return rfc,[CV_rfc.best_params_]\n",
    "\n",
    "# Grid to search for the best ElasticNet model\n",
    "def l2_l1_grid(X,y):\n",
    "    parametersGrid = {\"max_iter\": [1_000, 5_000],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0,1.0,0.1)}\n",
    "\n",
    "    eNet = ElasticNet()\n",
    "    grid = GridSearchCV(eNet, parametersGrid,verbose=1, scoring='r2', cv=5)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    l2l1 = ElasticNet(**grid.best_params_)\n",
    "    l2l1.fit(X,y)\n",
    "    return l2l1\n",
    "\n",
    "\n",
    "# Grid to search for the best KNN model\n",
    "def knn_grid(X,y):\n",
    "    params = {'n_neighbors':[3,5,9,13,15,31,51,61,75],\n",
    "              'weights':['uniform', 'distance']}\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "     estimator=KNeighborsRegressor(),\n",
    "     param_grid=params,\n",
    "     verbose=1,\n",
    "     scoring=\"neg_mean_squared_error\",\n",
    "     return_train_score=True\n",
    "     )\n",
    "    \n",
    "    grid.fit(X,y)\n",
    "    knn=KNeighborsRegressor(**grid.best_params_)\n",
    "    knn.fit(X,y)\n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Returns the linear regresion and its test rmse\n",
    "def first_test(X,y):\n",
    "    print(\"LinearRegression\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=153 )\n",
    "    linreg = linear_model.LinearRegression()\n",
    "    linreg.fit(X_train, y_train)\n",
    "    y_pred = linreg.predict(X_train)\n",
    "    \n",
    "    y_pred = linreg.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"RMSE (test) =\",rmse)\n",
    "    return linreg, rmse\n",
    "\n",
    "# Returns the best Random Forest model and its test rmse\n",
    "def forest_fit(X,y):\n",
    "    print(\"Forest\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=153)\n",
    "    func = random_tree_grid(X_train, y_train)\n",
    "    y_pred = func.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"RMSE (test) =\",rmse)\n",
    "    return func, rmse\n",
    "\n",
    "# Returns the best ElasticNet model and its test rmse\n",
    "def l2l1_fit(X,y):\n",
    "    print(\"L2 L1\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=153)\n",
    "    func = l2_l1_grid(X_train, y_train)\n",
    "    y_pred = func.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"RMSE (test) =\",rmse)\n",
    "    return func, rmse\n",
    "\n",
    "# Returns the best KNN model and its test rmse\n",
    "def knn_fit(X,y):\n",
    "    print(\"KNN\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=153)\n",
    "    func = knn_grid(X_train, y_train)\n",
    "    y_pred = func.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"RMSE (test) =\",rmse)\n",
    "    return func, rmse\n",
    "\n",
    "\n",
    "\n",
    "# Improved function to obtain the best fitting Random Forest for a greater grid\n",
    "def forest_tuning_fit(X,y):\n",
    "    print(\"Forest\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=153)\n",
    "    func,bf = tuning_random_tree_grid(X_train, y_train)\n",
    "    y_pred = func.predict(X_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "    print(\"RMSE (test) =\",rmse)\n",
    "    return func, rmse,bf\n",
    "\n",
    "# Function to obtain the best fitting ML method ant its test RMSE\n",
    "def best_func(X,y):\n",
    "    best_one = None\n",
    "    best_rmse = 1_000_000\n",
    "    #lambda a,b: l2l1_fit(a,b),\n",
    "    f_list=[lambda a,b: first_test(a,b),\n",
    "            lambda a,b: knn_fit(a,b),\n",
    "            lambda a,b: forest_fit(a,b) ]\n",
    "    \n",
    "    for func in f_list:\n",
    "        fnc,rmse = func(X,y)\n",
    "        if rmse < best_rmse:\n",
    "            print(\"Choosen one best\",rmse, best_rmse,rmse < best_rmse)\n",
    "            best_rmse=rmse\n",
    "            best_one=fnc\n",
    "    \n",
    "    return best_one,best_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LinearRegression\n",
      "RMSE (test) = 1363.1867503555823\n",
      "L2 L1\n",
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26666402935.555344, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26862124014.386856, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26714252404.50979, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26916668442.03939, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26735482398.913624, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26666402935.555267, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26862124014.38685, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26714252404.509678, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26916668442.03956, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 26735482398.913532, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27519726669.684345, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27715618909.535236, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27559971487.462914, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27771074602.726547, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27589624355.988552, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27519726669.68423, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27715618909.535442, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27559971487.46302, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27771074602.7265, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 27589624355.988514, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35638123712.89513, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35835546503.99339, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35605506610.31789, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35899491712.59746, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35713297076.5062, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35638123712.89513, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35835546503.99343, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35605506610.31783, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35899491712.59752, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 35713297076.506325, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89480925142.20459, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89682492870.34622, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88940690122.05615, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89794397150.74701, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89475214588.99243, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89480925142.2045, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89682492870.34628, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 88940690122.0561, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89794397150.74707, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 89475214588.99223, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182741141110.34067, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182870859538.04184, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 181198285440.52048, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 183028265787.3359, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182076800122.76376, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182741141110.34042, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182870859538.04224, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 181198285440.52032, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 183028265787.33588, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 182076800122.76373, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213744587979.3843, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213710237632.30527, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 211840119114.02664, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213877967770.5753, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 212625991665.01874, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213744587979.384, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213710237632.30505, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 211840119114.02655, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 213877967770.57547, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 212625991665.01874, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218675040185.15594, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218582112686.23917, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 216725511473.59546, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218759704432.76868, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 217460549531.6359, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218675040185.15598, tolerance: 43857672.21343696\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218582112686.2391, tolerance: 43837395.2848854\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 216725511473.59552, tolerance: 43466785.02641014\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 218759704432.76865, tolerance: 43873272.38201038\n",
      "  positive)\n",
      "C:\\Users\\marin\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 217460549531.63586, tolerance: 43612299.053458884\n",
      "  positive)\n",
      "RMSE (test) = 1363.1673045366379\n",
      "Forest\n",
      "Fitting 5 folds for each of 336 candidates, totalling 1680 fits\n",
      "RMSE (test) = 563.5919480419145\n",
      "KNN\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "RMSE (test) = 680.7374333584737\n",
      "Forest\n",
      "Fitting 6 folds for each of 432 candidates, totalling 2592 fits\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.978 total time=  11.3s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.978 total time=   2.7s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.975 total time=   2.7s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.978 total time=   2.7s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.975 total time=   2.8s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=50;, score=0.975 total time=   2.7s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.978 total time=  11.9s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.978 total time=  11.4s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.976 total time=  11.2s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.978 total time=  11.0s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.975 total time=  11.4s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=250;, score=0.975 total time=  12.0s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.978 total time=  45.7s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.978 total time=  43.0s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.976 total time=  43.7s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.978 total time=  44.0s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.975 total time=  41.1s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1000;, score=0.975 total time=  43.9s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.978 total time=  52.5s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.978 total time=  54.7s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.976 total time=  53.6s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.978 total time=  49.9s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.975 total time=  49.0s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1240;, score=0.975 total time=  50.2s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.978 total time= 1.0min\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.978 total time= 1.0min\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.976 total time= 1.0min\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.978 total time= 1.0min\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.975 total time=  58.7s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=1500;, score=0.975 total time= 1.0min\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.978 total time= 1.4min\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.978 total time= 1.4min\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.976 total time= 1.5min\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.978 total time= 1.5min\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.975 total time= 1.5min\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=2, n_estimators=2000;, score=0.975 total time= 1.4min\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.979 total time=   2.7s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.979 total time=   2.6s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.977 total time=   2.5s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.980 total time=   2.6s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.976 total time=   2.6s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=50;, score=0.976 total time=   2.6s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.979 total time=  10.4s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.979 total time=  10.3s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.977 total time=  10.8s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.980 total time=  10.2s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.976 total time=  10.3s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=250;, score=0.977 total time=  10.6s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.979 total time=  42.2s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.979 total time=  43.5s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.977 total time=  40.5s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.980 total time=  43.0s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.976 total time=  41.3s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1000;, score=0.977 total time=  43.0s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.979 total time=  52.7s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.979 total time=  49.7s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.977 total time=  49.9s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.980 total time=  54.0s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.976 total time=  53.0s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1240;, score=0.977 total time=  50.9s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.979 total time= 1.0min\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.979 total time= 1.1min\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.977 total time= 1.1min\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.980 total time= 1.1min\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.976 total time= 1.1min\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=1500;, score=0.977 total time= 1.1min\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.979 total time= 1.3min\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.979 total time= 1.4min\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.977 total time= 1.4min\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.980 total time= 1.4min\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.976 total time= 1.4min\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=5, n_estimators=2000;, score=0.977 total time= 1.4min\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.980 total time=   2.5s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.980 total time=   2.4s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.978 total time=   2.6s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.981 total time=   2.5s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.977 total time=   2.6s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=50;, score=0.977 total time=   2.4s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.980 total time=   9.9s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.980 total time=   9.7s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.978 total time=  10.0s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.981 total time=   9.7s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.977 total time=   9.8s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=250;, score=0.978 total time=  13.8s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.980 total time=  38.0s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.980 total time=  39.4s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.978 total time=  37.6s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.981 total time=  37.4s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.977 total time=  37.4s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1000;, score=0.978 total time=  38.0s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.980 total time=  46.8s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.980 total time=  48.1s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.978 total time=  45.8s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.981 total time=  45.5s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.977 total time=  44.8s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1240;, score=0.978 total time=  45.7s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.980 total time=  54.9s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.980 total time=  55.4s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.978 total time=  54.7s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.981 total time=  54.8s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.977 total time=  56.0s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=1500;, score=0.978 total time=  54.6s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.980 total time= 1.2min\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.980 total time= 1.3min\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.978 total time= 1.2min\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.981 total time= 1.2min\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.977 total time= 1.2min\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=1, min_samples_split=10, n_estimators=2000;, score=0.978 total time= 1.2min\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.979 total time=   2.7s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.980 total time=   2.7s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.978 total time=   2.6s\n",
      "[CV 4/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.981 total time=   2.5s\n",
      "[CV 5/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.976 total time=   2.5s\n",
      "[CV 6/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=50;, score=0.977 total time=   2.6s\n",
      "[CV 1/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=250;, score=0.979 total time=  10.6s\n",
      "[CV 2/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=250;, score=0.980 total time=  10.2s\n",
      "[CV 3/6] END max_depth=25, max_features=auto, min_samples_leaf=3, min_samples_split=2, n_estimators=250;, score=0.978 total time=  10.7s\n"
     ]
    }
   ],
   "source": [
    "#Entrenamientos\n",
    "first_test(X,y)\n",
    "l2l1_fit(X,y)\n",
    "forest_fit(X,y)\n",
    "knn_fit(X,y)\n",
    "forest_tuning_fit(X,y)\n",
    "best_func(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = pd.read_csv(\"diamonds_test.csv\", index_col=0)\n",
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codi(X_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_x = np.array(X_pred[['carat','cut', 'color', 'clarity']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_submit = model.predict(pred_x)\n",
    "predictions_submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(\"sample_submission.csv\")\n",
    "submission = pd.DataFrame({\"id\": range(len(predictions_submit)), \"price\": predictions_submit})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from day4_competi_diamond.confeti_kaggle_apr import chequeator\n",
    "chequeator(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Grid to search for the best random tree model\n",
    "def random_tree_grid(X,y):\n",
    "    rfc = RandomForestRegressor(n_jobs=-1,max_features= 'sqrt'\n",
    "                                 ,n_estimators=50, criterion = \"mse\") \n",
    "    \n",
    "    param_grid = { \n",
    "        'n_estimators': [10,25,40,50,60,75,100],\n",
    "        'min_samples_split': [5,25,50,100],\n",
    "        'min_samples_leaf': [5,25,50,100],\n",
    "        'max_features': ['auto', 'sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    CV_rfc = GridSearchCV(estimator=rfc,verbose=1, param_grid=param_grid, cv= 5)\n",
    "    CV_rfc.fit(X, y)\n",
    "    \n",
    "    rfc = RandomForestRegressor(n_jobs=-1,**CV_rfc.best_params_, criterion = \"mse\")\n",
    "    rfc.fit(X,y)\n",
    "    \n",
    "    return rfc\n",
    "\n",
    "# Improved grid to search for the best random tree model\n",
    "def tuning_random_tree_grid(X,y):\n",
    "    rfc = RandomForestRegressor(n_jobs=-1,max_features= 'sqrt'\n",
    "                                 ,n_estimators=50, criterion = \"mse\") \n",
    "    \n",
    "    param_grid = { \n",
    "        'max_depth': [25,50,100,None],\n",
    "        'min_samples_split': [2,5,10],\n",
    "        'min_samples_leaf': [1,3,5],\n",
    "        'max_features': ['auto', 'sqrt'],\n",
    "        'n_estimators': [50,250, 1000, 1240, 1500, 2000]\n",
    "    }\n",
    " \n",
    "    CV_rfc = GridSearchCV(estimator=rfc,verbose=3, param_grid=param_grid, cv= 6)\n",
    "    CV_rfc.fit(X, y)\n",
    "    \n",
    "    rfc = RandomForestRegressor(n_jobs=-1,**CV_rfc.best_params_, criterion = \"mse\")\n",
    "    rfc.fit(X,y)\n",
    "    \n",
    "    return rfc,[CV_rfc.best_params_]\n",
    "\n",
    "# Grid to search for the best ElasticNet model\n",
    "def l2_l1_grid(X,y):\n",
    "    parametersGrid = {\"max_iter\": [1_000, 5_000],\n",
    "                      \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
    "                      \"l1_ratio\": np.arange(0.0,1.0,0.1)}\n",
    "\n",
    "    eNet = ElasticNet()\n",
    "    grid = GridSearchCV(eNet, parametersGrid,verbose=1, scoring='r2', cv=5)\n",
    "    grid.fit(X, y)\n",
    "    \n",
    "    l2l1 = ElasticNet(**grid.best_params_)\n",
    "    l2l1.fit(X,y)\n",
    "    return l2l1\n",
    "\n",
    "\n",
    "# Grid to search for the best KNN model\n",
    "def knn_grid(X,y):\n",
    "    params = {'n_neighbors':[3,5,9,13,15,31,51,61,75],\n",
    "              'weights':['uniform', 'distance']}\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "     estimator=KNeighborsRegressor(),\n",
    "     param_grid=params,\n",
    "     verbose=1,\n",
    "     scoring=\"neg_mean_squared_error\",\n",
    "     return_train_score=True\n",
    "     )\n",
    "    \n",
    "    grid.fit(X,y)\n",
    "    knn=KNeighborsRegressor(**grid.best_params_)\n",
    "    knn.fit(X,y)\n",
    "    return knn"
   ]
  }
 ]
}